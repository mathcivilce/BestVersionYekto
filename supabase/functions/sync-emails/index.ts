/**
 * ====================================================================================================
 * EMAIL SYNC EDGE FUNCTION - COMPREHENSIVE ENTERPRISE EMAIL SYNCHRONIZATION SYSTEM
 * ====================================================================================================
 * 
 * This is the core email synchronization function for the SaaS email management platform.
 * It handles multi-store email synchronization with enterprise-grade features:
 * 
 * CORE FEATURES:
 * ‚úÖ Date range filtering for targeted sync operations
 * ‚úÖ Automatic token refresh when Microsoft tokens expire
 * ‚úÖ Universal RFC2822 threading system (platform independent)
 * ‚úÖ Smart Reference Architecture for attachment handling
 * ‚úÖ Comprehensive error handling and timeout protection
 * ‚úÖ Rate limit management with exponential backoff
 * ‚úÖ Database overwhelm protection with safety limits
 * ‚úÖ Detailed debugging and error classification
 * ‚úÖ SYNTHETIC ATTACHMENT PROCESSING for orphaned CIDs
 * ‚úÖ ENHANCED STRATEGY 1-3 with comprehensive debugging
 * ‚úÖ EDGE CASE PROTECTION for Strategy 2 conflicts
 * ‚úÖ ADVANCED CID MATCHING with field analysis
 * ‚úÖ SMART DIRECTION DETECTION for inbound/outbound emails
 * 
 * THREADING EVOLUTION:
 * - Phase 1: Basic Microsoft conversationId (unreliable)
 * - Phase 2: Microsoft Conversation API integration
 * - Phase 3: Universal RFC2822 threading system (current) - Platform independent, no extra API calls
 * 
 * PERFORMANCE IMPROVEMENTS (Phase 3):
 * - Eliminated Microsoft Conversation API calls completely (~70% faster)
 * - Enhanced threading accuracy and consistency
 * - Platform-independent threading logic
 * - Superior internal notes system integration
 * 
 * CRITICAL: This function has been extensively tested and is working correctly.
 * Any modifications should be thoroughly tested to prevent breaking the email sync flow.
 * ====================================================================================================
 */ import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
 import { createClient } from "npm:@supabase/supabase-js";
 import { Client } from "npm:@microsoft/microsoft-graph-client";
 import { createEmailProvider, getProviderTypeFromPlatform, AttachmentProcessor } from "../_shared/email-providers-sync-emails.ts";
 import { CidDetectionEngine } from "../_shared/cid-detection-engine.ts";
 import { SyntheticAttachmentProcessor } from "../_shared/synthetic-attachment-processor.ts";
 import { initializeGlobalMonitoring } from "../_shared/monitoring-synthetic.ts";
 // ====================================================================================================
 // CONFIGURATION AND CONSTANTS
 // ====================================================================================================
 /**
  * CORS headers for cross-origin requests
  * Required for frontend to communicate with Edge Function
  */ const corsHeaders = {
   'Access-Control-Allow-Origin': '*',
   'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
   'Content-Type': 'application/json'
 };
 /**
  * Performance and reliability constants
  * These values have been optimized for Microsoft Graph API limits and database performance
  */ const PAGE_SIZE = 50; // Number of emails to fetch per API call (optimal for Graph API)
 const BATCH_SIZE = 20; // Number of emails to save per database transaction (prevents timeouts)
 const MAX_RETRIES = 3; // Maximum retry attempts for failed operations
 const RETRY_DELAY = 2000; // Initial delay between retries (2 seconds, with exponential backoff)
 // ====================================================================================================
 // UTILITY FUNCTIONS
 // ====================================================================================================
 /**
  * Extract specific header value from internetMessageHeaders array
  * This is crucial for RFC2822 threading - extracts headers like Message-ID, In-Reply-To, References
  */ function extractHeader(headers, headerName) {
   if (!headers || !Array.isArray(headers)) return null;
   const header = headers.find((h)=>h.name && h.name.toLowerCase() === headerName.toLowerCase());
   return header?.value || null;
 }
 /**
  * Extract multiple Message-IDs from References header
  * The References header contains the complete conversation history
  */ function extractReferences(referencesHeader) {
   if (!referencesHeader) return null;
   // References header contains space-separated Message-IDs in angle brackets
   return referencesHeader.trim();
 }
 /**
  * üåç UNIVERSAL RFC2822 HEADER EXTRACTION
  * Extract embedded RFC2822 headers from email content for universal threading
  */ function extractEmbeddedRFC2822Headers(htmlContent) {
   if (!htmlContent) return {};
   try {
     // Look for our embedded RFC2822 headers block
     const headerBlockMatch = htmlContent.match(/<!--\[RFC2822-THREADING-HEADERS-START\]-->(.*?)<!--\[RFC2822-THREADING-HEADERS-END\]-->/s);
     if (!headerBlockMatch) return {};
     const headerBlock = headerBlockMatch[1];
     const headers = {};
     // Extract individual headers using regex
     const messageIdMatch = headerBlock.match(/Message-ID:\s*([^\n\r]+)/);
     if (messageIdMatch) headers.messageId = messageIdMatch[1].trim();
     const inReplyToMatch = headerBlock.match(/In-Reply-To:\s*([^\n\r]+)/);
     if (inReplyToMatch) headers.inReplyTo = inReplyToMatch[1].trim();
     const referencesMatch = headerBlock.match(/References:\s*([^\n\r]+)/);
     if (referencesMatch) headers.references = referencesMatch[1].trim();
     const threadTopicMatch = headerBlock.match(/Thread-Topic:\s*([^\n\r]+)/);
     if (threadTopicMatch) headers.threadTopic = threadTopicMatch[1].trim();
     const threadIndexMatch = headerBlock.match(/Thread-Index:\s*([^\n\r]+)/);
     if (threadIndexMatch) headers.threadIndex = threadIndexMatch[1].trim();
     console.log('üåç Extracted embedded RFC2822 headers:', Object.keys(headers));
     return headers;
   } catch (error) {
     console.error('Error extracting embedded RFC2822 headers:', error);
     return {};
   }
 }
 /**
  * Generic retry operation with exponential backoff
  */ async function retryOperation(operation, retries = MAX_RETRIES, delay = RETRY_DELAY) {
   try {
     return await operation();
   } catch (error) {
     // Only retry on rate limiting (429) or server errors (5xx)
     if (retries > 0 && (error.statusCode === 429 || error.statusCode >= 500)) {
       console.log(`Retrying operation, ${retries} attempts remaining`);
       await new Promise((resolve)=>setTimeout(resolve, delay));
       return retryOperation(operation, retries - 1, delay * 2);
     }
     throw error;
   }
 }
 // ====================================================================================================
 // SYNTHETIC ATTACHMENT PROCESSING FUNCTION
 // ====================================================================================================
 /**
  * Process synthetic attachments for orphaned CIDs in email batch
  */ async function processSyntheticAttachments(batch, supabase, userId, batchNumber) {
   console.log(`üîß [SYNTHETIC] Processing batch ${batchNumber} with ${batch.length} emails for orphaned CIDs`);
   try {
     // Initialize synthetic processing components
     const monitoring = initializeGlobalMonitoring(supabase);
     const cidEngine = new CidDetectionEngine(supabase);
     const syntheticProcessor = new SyntheticAttachmentProcessor(supabase, monitoring, {
       maxSyntheticPerEmail: 3,
       batchSize: 10,
       enableValidation: true,
       persistToDatabase: true,
       skipLowConfidence: true,
       confidenceThreshold: 50
     });
     // Convert batch to EmailRecord format for CID detection
     const emailRecords = batch.map((email)=>({
         id: email.id,
         content: email.content || '',
         graph_id: email.graph_id,
         has_attachments: email.has_attachments || false,
         attachment_reference_count: email.attachment_reference_count || 0,
         subject: email.subject,
         created_at: email.created_at
       }));
     // Detect orphaned CIDs in the batch
     const detectionOperationId = monitoring.startOperation('detection');
     const { orphanedEmails, stats } = await cidEngine.detectOrphanedCidsBatch(emailRecords);
     // Update monitoring with detection results
     monitoring.updateOperation(detectionOperationId, {
       emailsProcessed: emailRecords.length,
       syntheticAttachmentsCreated: 0,
       resolutionAttempts: 0,
       successfulResolutions: 0,
       errors: 0
     });
     monitoring.completeOperation(detectionOperationId);
     monitoring.recordCidDetection(stats);
     // Process orphaned emails if any found
     if (orphanedEmails.length > 0) {
       console.log(`üîß [SYNTHETIC] Found ${orphanedEmails.length} emails with orphaned CIDs in batch ${batchNumber}`);
       const batchResult = await syntheticProcessor.processBatch(orphanedEmails);
       console.log(`‚úÖ [SYNTHETIC] Batch ${batchNumber} processing complete:`, {
         totalEmails: batchResult.totalEmails,
         syntheticAttachments: batchResult.syntheticAttachmentsCreated,
         errors: batchResult.errors.length,
         duration: `${batchResult.processingTimeMs}ms`
       });
       // Log session summary
       monitoring.logSessionSummary();
     } else {
       console.log(`‚ÑπÔ∏è [SYNTHETIC] No orphaned CIDs found in batch ${batchNumber}`);
     }
   } catch (error) {
     console.error(`üö´ [SYNTHETIC] Error processing synthetic attachments for batch ${batchNumber}:`, {
       error: error.message,
       stack: error.stack
     });
     throw error;
   }
 }
 // ====================================================================================================
 // MAIN EDGE FUNCTION
 // ====================================================================================================
 serve(async (req)=>{
   // Handle CORS preflight requests
   if (req.method === 'OPTIONS') {
     return new Response('ok', {
       headers: corsHeaders
     });
   }
   // ================================================================================================
   // INITIALIZATION AND DEBUG SETUP
   // ================================================================================================
   const startTime = Date.now();
   /**
    * Debug information object - tracks sync progress and identifies failure points
    */ let debugInfo = {
     requestReceived: new Date().toISOString(),
     parametersExtracted: false,
     dateRangeApplied: false,
     tokenValidated: false,
     emailsFetched: 0,
     pagesProcessed: 0,
     databaseBatchesProcessed: 0,
     totalDuration: 0,
     failureReason: null
   };
   try {
     // ==============================================================================================
     // REQUEST PARAMETER EXTRACTION AND VALIDATION
     // ==============================================================================================
     const requestBody = await req.json();
     const { storeId, syncFrom, syncTo, // Chunk parameters from background-sync-processor
     chunkId, chunkIndex, totalChunks, startOffset, endOffset, estimatedEmails, chunked = false, parentSyncJobId } = requestBody;
     debugInfo.parametersExtracted = true;
     // Log all received parameters for debugging
     console.log('=== REQUEST PARAMETERS DEBUG ===');
     console.log('Full request body:', JSON.stringify(requestBody, null, 2));
     console.log('storeId:', storeId);
     console.log('syncFrom:', syncFrom);
     console.log('syncTo:', syncTo);
     console.log('chunked:', chunked);
     if (chunked) {
       console.log('üß© CHUNK PROCESSING MODE:');
       console.log('  chunkId:', chunkId);
       console.log('  chunkIndex:', chunkIndex);
       console.log('  totalChunks:', totalChunks);
       console.log('  startOffset:', startOffset);
       console.log('  endOffset:', endOffset);
       console.log('  estimatedEmails:', estimatedEmails);
     }
     console.log('=== END PARAMETERS DEBUG ===');
     // Validate required storeId parameter
     if (!storeId) {
       debugInfo.failureReason = 'MISSING_STORE_ID';
       return new Response(JSON.stringify({
         error: 'Store ID is required',
         debugInfo
       }), {
         status: 400,
         headers: corsHeaders
       });
     }
     console.log(`Starting email sync for store: ${storeId}`);
     // ==============================================================================================
     // SUPABASE CLIENT INITIALIZATION
     // ==============================================================================================
     const supabase = createClient(Deno.env.get('SUPABASE_URL') ?? '', Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? '', {
       auth: {
         autoRefreshToken: false,
         persistSession: false
       }
     });
     // ==============================================================================================
     // STORE DETAILS RETRIEVAL
     // ==============================================================================================
     const { data: store, error: storeError } = await supabase.from('stores').select('*').eq('id', storeId).single();
     if (storeError) {
       debugInfo.failureReason = 'STORE_NOT_FOUND';
       throw storeError;
     }
     let accessToken = store.access_token;
     // ==============================================================================================
     // TOKEN MANAGEMENT AND REFRESH LOGIC
     // ==============================================================================================
     const refreshTokenIfNeeded = async ()=>{
       console.log('Attempting to refresh token...');
       const refreshResponse = await fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/refresh-tokens`, {
         method: 'POST',
         headers: {
           'Content-Type': 'application/json',
           'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`
         },
         body: JSON.stringify({
           storeId
         })
       });
       if (!refreshResponse.ok) {
         debugInfo.failureReason = 'TOKEN_REFRESH_FAILED';
         throw new Error(`Token refresh failed: ${refreshResponse.status}`);
       }
       const refreshResult = await refreshResponse.json();
       if (!refreshResult.success) {
         debugInfo.failureReason = 'TOKEN_REFRESH_ERROR';
         throw new Error(refreshResult.error || 'Token refresh failed');
       }
       // Get the updated token from database
       const { data: updatedStore, error: updateError } = await supabase.from('stores').select('access_token').eq('id', storeId).single();
       if (updateError) {
         debugInfo.failureReason = 'TOKEN_UPDATE_ERROR';
         throw updateError;
       }
       accessToken = updatedStore.access_token;
       console.log('Token refreshed successfully');
       return accessToken;
     };
     const createGraphClient = (token)=>{
       return Client.init({
         authProvider: (done)=>{
           done(null, token);
         }
       });
     };
     const testTokenWithRetry = async (maxRetries = 1)=>{
       for(let attempt = 0; attempt <= maxRetries; attempt++){
         try {
           const testClient = createGraphClient(accessToken);
           await testClient.api('/me').get();
           console.log('Token validation successful');
           debugInfo.tokenValidated = true;
           return;
         } catch (error) {
           console.error(`Token test attempt ${attempt + 1} failed:`, error.statusCode);
           if (error.statusCode === 401 && attempt < maxRetries) {
             await refreshTokenIfNeeded();
           } else {
             debugInfo.failureReason = 'TOKEN_VALIDATION_FAILED';
             throw new Error(`Token validation failed: ${error.message}`);
           }
         }
       }
     };
     // Validate token before proceeding
     await testTokenWithRetry(1);
     // ==============================================================================================
     // SYNC VARIABLES INITIALIZATION
     // ==============================================================================================
     const allEmails = [];
     let emailsToProcess = []; // ‚úÖ CRITICAL FIX: Declare emailsToProcess variable
     let pageCount = 0;
     let nextLink = null;
     let conversationFetchAttempts = 0;
     let conversationFetchSuccesses = 0;
     let conversationFetchFailures = 0;
     // ==============================================================================================
     // DATE RANGE FILTERING SETUP (CRITICAL FIX)
     // ==============================================================================================
     let filter = "isDraft eq false";
     if (syncFrom || syncTo) {
       console.log('=== DATE RANGE FILTERING DEBUG ===');
       console.log('Original syncFrom:', syncFrom);
       console.log('Original syncTo:', syncTo);
       if (syncFrom) {
         const fromDate = new Date(syncFrom);
         // For syncFrom, start at the beginning of the day (00:00:00.000Z)
         fromDate.setUTCHours(0, 0, 0, 0);
         const fromIsoString = fromDate.toISOString();
         filter += ` and receivedDateTime ge ${fromIsoString}`;
         console.log('Added FROM filter:', `receivedDateTime ge ${fromIsoString}`);
       }
       if (syncTo) {
         const toDate = new Date(syncTo);
         // For syncTo, end at the end of the day (23:59:59.999Z)
         toDate.setUTCHours(23, 59, 59, 999);
         const toIsoString = toDate.toISOString();
         filter += ` and receivedDateTime le ${toIsoString}`;
         console.log('Added TO filter:', `receivedDateTime le ${toIsoString}`);
       }
       debugInfo.dateRangeApplied = true;
       console.log('Final Microsoft Graph filter:', filter);
       console.log('=== END DATE RANGE FILTERING DEBUG ===');
     } else {
       console.log('‚ö†Ô∏è  NO DATE RANGE PROVIDED - Fetching ALL emails (potentially dangerous!)');
     }
     // ==============================================================================================
     // TIMEOUT PROTECTION SETUP
     // ==============================================================================================
     const maxExecutionTime = 4.5 * 60 * 1000; // 4.5 minutes
     const executionStartTime = Date.now();
     const checkTimeout = ()=>{
       const elapsed = Date.now() - executionStartTime;
       if (elapsed > maxExecutionTime) {
         debugInfo.failureReason = 'EXECUTION_TIMEOUT';
         throw new Error(`Function timeout approaching: ${elapsed}ms elapsed, max: ${maxExecutionTime}ms`);
       }
       return elapsed;
     };
         // ==============================================================================================
    // CHUNKED VS NON-CHUNKED EMAIL FETCHING STRATEGY
    // ==============================================================================================
    
    if (chunked) {
      console.log(`üß© [CHUNKED-MODE] Processing chunk ${chunkIndex}/${totalChunks}`);
      console.log(`üß© [CHUNK-BOUNDARIES] Start: ${startOffset}, End: ${endOffset}, Estimated: ${estimatedEmails}`);
    } else {
      console.log(`üìã [SINGLE-MODE] Processing complete mailbox sync`);
    }

    if (chunked && typeof startOffset === 'number' && typeof endOffset === 'number') {
      // üß© CHUNKED MODE: Fetch specific email range using Graph API pagination
      console.log(`üß© [CHUNKED-FETCH] Fetching emails for chunk ${chunkIndex}/${totalChunks}`);
      
      const chunkSize = endOffset - startOffset + 1;
      console.log(`üß© [CHUNK-SIZE] Processing ${chunkSize} emails (range ${startOffset}-${endOffset})`);
      
      // Calculate how many pages to skip to reach startOffset
      const emailsPerPage = PAGE_SIZE;
      const pagesToSkip = Math.floor(startOffset / emailsPerPage);
      const offsetWithinPage = startOffset % emailsPerPage;
      
      console.log(`üß© [PAGINATION] Skipping ${pagesToSkip} pages, offset within page: ${offsetWithinPage}`);
      
      // Build Graph API query with skip parameter
      let graphQuery = `/me/messages`;
      const queryParams = [];
      
      // Add filter for date range and draft exclusion
      queryParams.push(`$filter=${encodeURIComponent(filter)}`);
      
      // Add skip parameter for chunk positioning
      if (pagesToSkip > 0) {
        queryParams.push(`$skip=${pagesToSkip * emailsPerPage}`);
      }
      
      // Add other parameters
      queryParams.push(`$select=id,subject,bodyPreview,from,toRecipients,receivedDateTime,isRead,body,conversationId,internetMessageId,parentFolderId,internetMessageHeaders,hasAttachments`);
      queryParams.push(`$orderby=receivedDateTime desc`);
      queryParams.push(`$top=${emailsPerPage}`);
      
      const fullQuery = `${graphQuery}?${queryParams.join('&')}`;
      console.log(`üß© [GRAPH-QUERY] ${fullQuery.substring(0, 200)}...`);
      
      // Fetch emails for this specific chunk
      let emailsFetched = 0;
      let currentPageOffset = offsetWithinPage;
      
      try {
        do {
          pageCount++;
          debugInfo.pagesProcessed = pageCount;
          const elapsed = checkTimeout();
          console.log(`üìÑ [CHUNK-PAGE] Fetching page ${pageCount} for chunk ${chunkIndex}... (${elapsed}ms elapsed)`);
          
          const graphClient = createGraphClient(accessToken);
          let response;
          
          if (nextLink) {
            console.log('üìÑ [NEXT-PAGE] Using continuation token...');
            response = await retryOperation(() => graphClient.api(nextLink).get());
          } else {
            console.log('üìÑ [FIRST-PAGE] Fetching first page of chunk...');
            response = await retryOperation(() => graphClient.api(fullQuery).get());
          }
          
          if (!response || !Array.isArray(response.value)) {
            console.error('‚ùå [GRAPH-ERROR] Invalid response format:', response);
            debugInfo.failureReason = 'INVALID_GRAPH_RESPONSE';
            throw new Error('Invalid response format from Microsoft Graph API');
          }
          
          // Process emails from this page, respecting chunk boundaries
          let pageEmails = response.value;
          
          // If this is the first page and we have an offset within the page, skip those emails
          if (pageCount === 1 && currentPageOffset > 0) {
            pageEmails = pageEmails.slice(currentPageOffset);
            console.log(`üìÑ [PAGE-OFFSET] Skipped ${currentPageOffset} emails from first page`);
          }
          
          // Add emails to our collection, but don't exceed chunk boundaries
          for (const email of pageEmails) {
            if (emailsFetched >= chunkSize) {
              console.log(`üìÑ [CHUNK-LIMIT] Reached chunk size limit (${chunkSize}), stopping fetch`);
              nextLink = null; // Stop pagination
              break;
            }
            
            // Process email with threading headers (existing logic)
            const embeddedHeaders = extractEmbeddedRFC2822Headers(email.body?.content || '');
            const messageIdHeader = embeddedHeaders.messageId || extractHeader(email.internetMessageHeaders, 'X-Message-ID-RFC2822') || extractHeader(email.internetMessageHeaders, 'Message-ID') || email.internetMessageId;
            const inReplyToHeader = embeddedHeaders.inReplyTo || extractHeader(email.internetMessageHeaders, 'X-In-Reply-To-RFC2822') || extractHeader(email.internetMessageHeaders, 'In-Reply-To');
            const referencesHeader = embeddedHeaders.references || extractHeader(email.internetMessageHeaders, 'X-References-RFC2822') || extractReferences(extractHeader(email.internetMessageHeaders, 'References'));
            const threadIndexHeader = embeddedHeaders.threadIndex || extractHeader(email.internetMessageHeaders, 'X-Thread-Index') || extractHeader(email.internetMessageHeaders, 'Thread-Index');
            
            const enhancedEmail = {
              ...email,
              microsoft_conversation_id: email.conversationId,
              has_attachments: email.hasAttachments,
              body_preview: email.bodyPreview,
              received_date_time: email.receivedDateTime,
              message_id_header: messageIdHeader,
              in_reply_to_header: inReplyToHeader,
              references_header: referencesHeader,
              processed_by_custom_threading: true
            };
            
            allEmails.push(enhancedEmail);
            emailsFetched++;
            
            if (email.conversationId) {
              conversationFetchAttempts++;
              conversationFetchSuccesses++;
            }
          }
          
          // Get next page link if we haven't reached our chunk limit
          if (emailsFetched < chunkSize) {
            nextLink = response['@odata.nextLink'];
          } else {
            nextLink = null;
          }
          
          console.log(`üìÑ [CHUNK-PROGRESS] Fetched ${emailsFetched}/${chunkSize} emails for chunk ${chunkIndex}`);
          
          if (nextLink) {
            await new Promise((resolve)=>setTimeout(resolve, 2000));
          }
          
        } while (nextLink && emailsFetched < chunkSize);
        
        console.log(`‚úÖ [CHUNK-COMPLETE] Chunk ${chunkIndex} fetched ${emailsFetched} emails`);
        emailsToProcess = allEmails; // All emails are already within chunk boundaries
        debugInfo.emailsFetched = allEmails.length;
        
      } catch (error) {
        console.error(`‚ùå [CHUNK-FETCH-ERROR] Error fetching chunk ${chunkIndex}:`, error);
        if (error.statusCode === 429 || error.response && error.response.status === 429) {
          debugInfo.failureReason = 'RATE_LIMIT_EXCEEDED';
          const retryAfter = parseInt(error.headers?.get('Retry-After') || '60');
          console.log(`üö´ RATE LIMITED: waiting ${retryAfter} seconds...`);
          await new Promise((resolve)=>setTimeout(resolve, retryAfter * 1000));
        } else if (error.statusCode >= 500) {
          debugInfo.failureReason = 'MICROSOFT_SERVER_ERROR';
        } else if (error.message.includes('timeout')) {
          debugInfo.failureReason = 'NETWORK_TIMEOUT';
        } else {
          debugInfo.failureReason = 'UNKNOWN_GRAPH_ERROR';
        }
        throw error;
      }
      
    } else {
      // üìã NON-CHUNKED MODE: Keep existing logic for full syncs
      console.log('üìã [NON-CHUNKED] Fetching all emails with existing pagination logic');
      
      do {
        pageCount++;
        debugInfo.pagesProcessed = pageCount;
        const elapsed = checkTimeout();
        console.log(`üìÑ Fetching page ${pageCount}... (${elapsed}ms elapsed)`);
        try {
          const graphClient = createGraphClient(accessToken);
          let response;
          if (nextLink) {
            console.log('Fetching next page using continuation token...');
            response = await retryOperation(()=>graphClient.api(nextLink).get());
          } else {
            console.log('Fetching first page from Graph API...');
            console.log('üéØ Using filter:', filter);
            response = await retryOperation(()=>graphClient.api('/me/messages').filter(filter).select('id,subject,bodyPreview,from,toRecipients,receivedDateTime,isRead,body,conversationId,internetMessageId,parentFolderId,internetMessageHeaders').orderby('receivedDateTime desc').top(PAGE_SIZE).get());
          }
          if (!response || !Array.isArray(response.value)) {
            console.error('Invalid response format:', response);
            debugInfo.failureReason = 'INVALID_GRAPH_RESPONSE';
            throw new Error('Invalid response format from Microsoft Graph API');
          }
          if (response.value.length === 0 && pageCount === 1) {
            console.log('‚ö†Ô∏è  No emails returned on first page - check date range or email account');
          }
          
          // ============================================================================================
          // EMAIL PROCESSING WITH UNIVERSAL THREADING (PHASE 3)
          // ============================================================================================
          for (const email of response.value){
            const embeddedHeaders = extractEmbeddedRFC2822Headers(email.body?.content || '');
            const messageIdHeader = embeddedHeaders.messageId || extractHeader(email.internetMessageHeaders, 'X-Message-ID-RFC2822') || extractHeader(email.internetMessageHeaders, 'Message-ID') || email.internetMessageId;
            const inReplyToHeader = embeddedHeaders.inReplyTo || extractHeader(email.internetMessageHeaders, 'X-In-Reply-To-RFC2822') || extractHeader(email.internetMessageHeaders, 'In-Reply-To');
            const referencesHeader = embeddedHeaders.references || extractHeader(email.internetMessageHeaders, 'X-References-RFC2822') || extractReferences(extractHeader(email.internetMessageHeaders, 'References'));
            const threadIndexHeader = embeddedHeaders.threadIndex || extractHeader(email.internetMessageHeaders, 'X-Thread-Index') || extractHeader(email.internetMessageHeaders, 'Thread-Index');
            const enhancedEmail = {
              ...email,
              microsoft_conversation_id: email.conversationId,
              has_attachments: email.hasAttachments,
              body_preview: email.bodyPreview,
              received_date_time: email.receivedDateTime,
              message_id_header: messageIdHeader,
              in_reply_to_header: inReplyToHeader,
              references_header: referencesHeader,
              processed_by_custom_threading: true
            };
            allEmails.push(enhancedEmail);
            if (email.conversationId) {
              conversationFetchAttempts++;
              conversationFetchSuccesses++;
            }
          }
          debugInfo.emailsFetched = allEmails.length;
          nextLink = response['@odata.nextLink'];
          console.log(`Retrieved ${response.value.length} emails on page ${pageCount} (total: ${allEmails.length})`);
          
          // Safety stop for large datasets
          if (allEmails.length > 5000) {
            console.log(`‚ö†Ô∏è  SAFETY STOP: ${allEmails.length} emails collected. Stopping to prevent database overwhelm.`);
            debugInfo.failureReason = 'TOO_MANY_EMAILS_SAFETY_STOP';
            nextLink = null;
            break;
          }
          
          if (nextLink) {
            await new Promise((resolve)=>setTimeout(resolve, 2000));
          }
        } catch (error) {
          console.error('Error fetching emails:', {
            page: pageCount,
            status: error.statusCode || error.status,
            message: error.message,
            body: error.body
          });
          if (error.statusCode === 429 || error.response && error.response.status === 429) {
            debugInfo.failureReason = 'RATE_LIMIT_EXCEEDED';
            const retryAfter = parseInt(error.headers?.get('Retry-After') || '60');
            console.log(`üö´ RATE LIMITED: waiting ${retryAfter} seconds...`);
            await new Promise((resolve)=>setTimeout(resolve, retryAfter * 1000));
            continue;
          } else if (error.statusCode >= 500) {
            debugInfo.failureReason = 'MICROSOFT_SERVER_ERROR';
            console.log('üö´ Microsoft server error, stopping pagination');
            nextLink = null;
          } else if (error.message.includes('timeout')) {
            debugInfo.failureReason = 'NETWORK_TIMEOUT';
            console.log('üö´ Network timeout, stopping pagination');
            nextLink = null;
          } else {
            debugInfo.failureReason = 'UNKNOWN_GRAPH_ERROR';
            console.log('üö´ Unknown error, stopping pagination');
            nextLink = null;
          }
        }
      } while (nextLink);
      
      emailsToProcess = allEmails;
      console.log(`üìã [NON-CHUNKED] Processing all ${emailsToProcess.length} emails`);
    }
     console.log(`Total emails fetched: ${allEmails.length}`);
     // ==============================================================================================
     // EMAIL PROCESSING AND DATABASE STORAGE
     // ==============================================================================================
     if (emailsToProcess.length > 0) {
       if (emailsToProcess.length > 2000) {
         console.log(`‚ö†Ô∏è  LARGE DATASET WARNING: Processing ${emailsToProcess.length} emails - this may take a while`);
       }
       const emailsToSave = [];
       globalThis.supabaseClient = supabase;
       const providerType = getProviderTypeFromPlatform(store.platform || 'outlook');
       const emailProvider = createEmailProvider(providerType, store.id, accessToken);
       for (const msg of emailsToProcess){
         checkTimeout();
         const toEmails = msg.toRecipients?.map((r)=>r.emailAddress?.address).filter(Boolean).join(',') || '';
         const embeddedHeaders = extractEmbeddedRFC2822Headers(msg.body?.content || '');
         const threadIndexHeader = embeddedHeaders.threadIndex || extractHeader(msg.internetMessageHeaders, 'X-Thread-Index') || extractHeader(msg.internetMessageHeaders, 'Thread-Index');
         const { data: threadResult, error: threadError } = await supabase.rpc('get_or_create_thread_id_universal', {
           p_message_id_header: msg.message_id_header,
           p_in_reply_to_header: msg.in_reply_to_header,
           p_references_header: msg.references_header,
           p_subject: msg.subject || 'No Subject',
           p_from_email: msg.from?.emailAddress?.address || '',
           p_to_email: toEmails,
           p_date: msg.receivedDateTime || new Date().toISOString(),
           p_user_id: store.user_id,
           p_store_id: storeId,
           p_microsoft_conversation_id: msg.microsoft_conversation_id || msg.conversationId,
           p_thread_index_header: threadIndexHeader
         });
         if (threadError) {
           console.error('Error getting thread ID for message:', msg.id, threadError);
         }
         const universalThreadId = threadResult || msg.message_id_header || msg.microsoft_conversation_id || msg.conversationId;
         let attachmentCount = 0;
         const emailId = crypto.randomUUID();
         const emailRecord = {
           id: emailId,
           graph_id: msg.id,
           thread_id: universalThreadId,
           parent_id: null,
           subject: msg.subject || 'No Subject',
           from: msg.from?.emailAddress?.address || '',
           snippet: msg.body_preview || msg.bodyPreview || '',
           content: msg.body?.content || '',
           date: msg.received_date_time || msg.receivedDateTime || new Date().toISOString(),
           read: msg.isRead || false,
           priority: 1,
           status: 'open',
           store_id: storeId,
           user_id: store.user_id,
           business_id: store.business_id,
           internet_message_id: msg.internetMessageId,
           microsoft_conversation_id: msg.microsoft_conversation_id || msg.conversationId,
           has_attachments: attachmentCount > 0,
           attachment_reference_count: attachmentCount,
           message_id_header: msg.message_id_header,
           in_reply_to_header: msg.in_reply_to_header,
           references_header: msg.references_header,
           thread_index_header: threadIndexHeader,
           conversation_root_id: universalThreadId,
           processed_by_custom_threading: true,
           direction: (msg.from?.emailAddress?.address || '').toLowerCase() === store.email.toLowerCase() ? 'outbound' : 'inbound',
           recipient: toEmails
         };
         if (msg.hasAttachments || msg.has_attachments) {
           console.log(`üîç [ATTACHMENT-DEBUG] Processing email ${msg.id} - hasAttachments: ${msg.hasAttachments || msg.has_attachments}`);
           try {
             const attachmentMetadata = await emailProvider.extractAttachmentMetadata(msg.id);
             if (attachmentMetadata && attachmentMetadata.length > 0) {
               console.log(`üìé [ATTACHMENT-DEBUG] Found ${attachmentMetadata.length} raw attachments for email ${msg.id}`);
               const contentIds = AttachmentProcessor.extractContentIdFromHtml(msg.body?.content || '');
               console.log(`üîç [CID-EXTRACTION] Raw extracted CIDs:`, contentIds);
               const linkedAttachments = await AttachmentProcessor.linkContentIdsToAttachments(contentIds, attachmentMetadata);
               console.log(`üîó [CID-LINKING] Linked attachments result:`, linkedAttachments.length);
               attachmentCount = linkedAttachments.length;
               if (linkedAttachments.length > 0) {
                 emailRecord.pending_attachments = linkedAttachments;
                 console.log(`üì¶ [ATTACHMENT-DEBUG] Stored ${linkedAttachments.length} pending attachments for email ${msg.id}`);
               }
               emailRecord.has_attachments = attachmentCount > 0;
               emailRecord.attachment_reference_count = attachmentCount;
             }
           } catch (attachmentError) {
             console.error('üö´ [ATTACHMENT-ERROR] Error extracting attachments (non-fatal):', attachmentError);
           }
         }
         // Thread assignment inheritance
         if (universalThreadId) {
           try {
             const { data: existingThreadEmail } = await supabase.from('emails').select('assigned_to').eq('thread_id', universalThreadId).eq('user_id', store.user_id).not('assigned_to', 'is', null).order('date', {
               ascending: false
             }).limit(1).single();
             if (existingThreadEmail?.assigned_to) {
               emailRecord.assigned_to = existingThreadEmail.assigned_to;
               console.log('üîó Inheriting thread assignment for sync:', existingThreadEmail.assigned_to);
             }
           } catch (assignmentError) {
             console.log('‚ÑπÔ∏è No existing thread assignment found (this is normal for new threads)');
           }
         }
         emailsToSave.push(emailRecord);
       }
       let savedCount = 0;
       for(let i = 0; i < emailsToSave.length; i += BATCH_SIZE){
         checkTimeout();
         const batch = emailsToSave.slice(i, i + BATCH_SIZE);
         const batchNumber = Math.floor(i / BATCH_SIZE) + 1;
         const totalBatches = Math.ceil(emailsToSave.length / BATCH_SIZE);
         debugInfo.databaseBatchesProcessed = batchNumber;
         console.log(`Saving batch ${batchNumber} of ${totalBatches}`);
         try {
           await retryOperation(async ()=>{
             const { error: saveError } = await supabase.from('emails').upsert(batch, {
               onConflict: 'graph_id,user_id',
               ignoreDuplicates: false
             });
             if (saveError) {
               if (saveError.message.includes('duplicate') || saveError.message.includes('conflict')) {
                 console.log('üîÑ Attempting alternative upsert for sent emails...');
                 const emailsWithGraphId = batch.filter((email)=>email.graph_id);
                 const emailsWithoutGraphId = batch.filter((email)=>!email.graph_id);
                 if (emailsWithGraphId.length > 0) {
                   const { error: graphIdError } = await supabase.from('emails').upsert(emailsWithGraphId, {
                     onConflict: 'graph_id,user_id',
                     ignoreDuplicates: false
                   });
                   if (graphIdError) throw graphIdError;
                 }
                 if (emailsWithoutGraphId.length > 0) {
                   const { error: sentEmailError } = await supabase.from('emails').upsert(emailsWithoutGraphId, {
                     onConflict: 'message_id_header,user_id,store_id',
                     ignoreDuplicates: false
                   });
                   if (sentEmailError) {
                     console.warn('‚ö†Ô∏è Batch upsert failed, trying individual upserts for sent emails...');
                     for (const email of emailsWithoutGraphId){
                       try {
                         const { data: existingEmail } = await supabase.from('emails').select('id, graph_id').eq('message_id_header', email.message_id_header).eq('user_id', email.user_id).eq('store_id', email.store_id).maybeSingle();
                         if (existingEmail) {
                           await supabase.from('emails').update({
                             graph_id: email.graph_id,
                             microsoft_conversation_id: email.microsoft_conversation_id,
                             date: email.date
                           }).eq('id', existingEmail.id);
                           console.log(`‚úÖ Updated existing sent email ${existingEmail.id} with sync data`);
                         } else {
                           await supabase.from('emails').insert(email);
                           console.log(`‚úÖ Inserted new email ${email.id}`);
                         }
                       } catch (individualError) {
                         console.error(`‚ùå Failed to process individual email:`, individualError);
                       }
                     }
                   }
                 }
               } else {
                 if (saveError.message.includes('timeout')) {
                   debugInfo.failureReason = 'DATABASE_TIMEOUT';
                 } else if (saveError.message.includes('connection')) {
                   debugInfo.failureReason = 'DATABASE_CONNECTION_ERROR';
                 } else {
                   debugInfo.failureReason = 'DATABASE_SAVE_ERROR';
                 }
                 throw saveError;
               }
             }
           });
           savedCount += batch.length;
           console.log(`Successfully saved ${savedCount} of ${emailsToSave.length} emails`);
           // Process attachments for this batch
           console.log(`üìé [ATTACHMENT-BATCH] Processing attachments for batch ${batchNumber}`);
           let attachmentProcessedCount = 0;
           let attachmentErrorCount = 0;
           for (const email of batch){
             if (email.pending_attachments && email.pending_attachments.length > 0) {
               console.log(`üìé [ATTACHMENT-PROCESSING] Starting processing for email ${email.id} with ${email.pending_attachments.length} attachments`);
               try {
                 await AttachmentProcessor.processAttachmentMetadata(email.pending_attachments, email.id, store.user_id, supabase);
                 attachmentProcessedCount += email.pending_attachments.length;
                 console.log(`‚úÖ [ATTACHMENT-SUCCESS] Successfully processed ${email.pending_attachments.length} attachment references for email ${email.id}`);
               } catch (attachmentError) {
                 attachmentErrorCount += email.pending_attachments.length;
                 console.error(`üö´ [ATTACHMENT-ERROR] Error saving attachment references for email ${email.id}:`, attachmentError);
               }
             }
           }
           console.log(`üìä [ATTACHMENT-BATCH-SUMMARY] Batch ${batchNumber} - Processed: ${attachmentProcessedCount}, Errors: ${attachmentErrorCount}`);
           // Process synthetic attachments
           console.log(`üîß [SYNTHETIC-BATCH] Starting orphaned CID detection for batch ${batchNumber}`);
           try {
             await processSyntheticAttachments(batch, supabase, store.user_id, batchNumber);
           } catch (syntheticError) {
             console.error(`üö´ [SYNTHETIC-ERROR] Error processing synthetic attachments for batch ${batchNumber}:`, syntheticError);
           }
           if (i + BATCH_SIZE < emailsToSave.length) {
             await new Promise((resolve)=>setTimeout(resolve, 2000));
           }
         } catch (error) {
           console.error(`üö´ Error saving batch ${batchNumber}:`, error);
           continue;
         }
       }
     }
         // ==============================================================================================
    // FINALIZATION AND STORE UPDATE
    // ==============================================================================================
    // üéØ FIXED CHUNK COMPLETION DETECTION
    let isLastChunk = false;
    let allChunksCompleted = false;

    if (chunked && chunkId && parentSyncJobId) {
      console.log(`üß© [CHUNK-COMPLETION] Processing completion for chunk ${chunkIndex}/${totalChunks}`);
      
      // ‚úÖ STEP 1: Mark current chunk as completed FIRST
      const { error: updateError } = await supabase
        .from('chunked_sync_jobs')
        .update({
          status: 'completed',
          completed_at: new Date().toISOString(),
          emails_processed: emailsToProcess.length,
          processing_time_ms: Date.now() - startTime
        })
        .eq('id', chunkId);
        
      if (updateError) {
        console.error(`‚ùå [CHUNK-UPDATE] Failed to mark chunk ${chunkIndex} as completed:`, updateError);
      } else {
        console.log(`‚úÖ [CHUNK-UPDATE] Chunk ${chunkIndex}/${totalChunks} marked as completed`);
      }
      
      // ‚úÖ STEP 2: Check completion status of ALL chunks for this parent job
      const { data: chunkStatus, error: chunkError } = await supabase
        .from('chunked_sync_jobs')
        .select('chunk_index, total_chunks, status')
        .eq('parent_sync_job_id', parentSyncJobId)
        .order('chunk_index');
        
      if (chunkError) {
        console.error(`‚ùå [CHUNK-STATUS] Error checking chunk status:`, chunkError);
        isLastChunk = false;
        allChunksCompleted = false;
      } else if (chunkStatus && chunkStatus.length > 0) {
        const totalChunks = chunkStatus[0].total_chunks;
        const completedChunks = chunkStatus.filter(c => c.status === 'completed').length;
        const pendingChunks = chunkStatus.filter(c => c.status === 'pending').length;
        const processingChunks = chunkStatus.filter(c => c.status === 'processing').length;
        const failedChunks = chunkStatus.filter(c => c.status === 'failed').length;
        
        console.log(`üß© [CHUNK-STATUS] Completion analysis:`, {
          totalChunks,
          completedChunks,
          pendingChunks,
          processingChunks,
          failedChunks,
          currentChunk: chunkIndex
        });
        
        // ‚úÖ CORRECT LOGIC: All chunks completed only when ALL are marked completed
        allChunksCompleted = completedChunks === totalChunks;
        isLastChunk = allChunksCompleted;
        
        console.log(`üß© [COMPLETION-STATUS] Chunk ${chunkIndex}/${totalChunks} completed. Progress: ${completedChunks}/${totalChunks}`);
        console.log(`üß© [FINAL-CHECK] All chunks completed: ${allChunksCompleted}`);
        
        if (allChunksCompleted) {
          console.log('üéâ [ALL-CHUNKS-DONE] All chunks completed! Proceeding with final sync updates...');
        } else {
          console.log(`‚è≥ [CHUNKS-REMAINING] ${totalChunks - completedChunks} chunks still pending/processing`);
        }
      } else {
        console.error(`‚ùå [NO-CHUNKS] No chunk status found for parent job ${parentSyncJobId}`);
        isLastChunk = false;
        allChunksCompleted = false;
      }
    } else if (!chunked) {
      // ‚úÖ TRUE NON-CHUNKED OPERATION
      isLastChunk = true;
      allChunksCompleted = true;
      console.log('üìã [NON-CHUNKED] Single operation - will update store status');
    } else {
      // ‚ùå MISSING CHUNK PARAMETERS - This shouldn't happen
      console.error('‚ö†Ô∏è [INVALID-CHUNK] Chunked mode but missing chunk parameters');
      console.error('Debug - chunked:', chunked, 'chunkId:', chunkId, 'parentSyncJobId:', parentSyncJobId);
      isLastChunk = false;
      allChunksCompleted = false;
    }
     // ==============================================================================================
     // CALCULATE COMPREHENSIVE METRICS (BEFORE CONDITIONAL BLOCKS)
     // ==============================================================================================
     const universalThreadingSuccessRate = conversationFetchAttempts > 0 ? (conversationFetchSuccesses / conversationFetchAttempts * 100).toFixed(1) : '100';
     let totalAttachmentsProcessed = 0;
     let emailsWithAttachments = 0;
     for (const email of allEmails){
       if (email.hasAttachments || email.has_attachments) {
         emailsWithAttachments++;
       }
     }
     // üéØ FINAL CHUNK PROCESSING: Only update database when ALL chunks are complete
     if (isLastChunk && allChunksCompleted) {
       console.log('üéâ [FINAL-CHUNK] All chunks completed! Metrics calculated, proceeding with comprehensive logging...');
       console.log('üìä [METRICS] Comprehensive metrics calculated for final sync completion');
     } else {
       console.log(`‚ÑπÔ∏è [CHUNK-PROGRESS] Chunk completed but not final chunk. Waiting for remaining chunks to complete...`);
     }
     debugInfo.totalDuration = Date.now() - startTime;
     // ==============================================================================================
     // COMPREHENSIVE METRICS AND SUCCESS RESPONSE (FINAL CHUNK ONLY)
     // ==============================================================================================
     // üéØ ENHANCED LOGGING: Show comprehensive metrics only for final chunk or single operations
     if (isLastChunk && allChunksCompleted) {
       // Metrics already calculated above - using existing variables
       console.log('=== üéâ SYNC COMPLETED SUCCESSFULLY (FINAL CHUNK) ===');
       console.log(`üìß Emails processed in this chunk: ${allEmails.length}`);
       console.log(`üìé Emails with attachments in this chunk: ${emailsWithAttachments}`);
       console.log(`üßµ Emails with conversation metadata: ${conversationFetchAttempts}`);
       console.log(`‚úÖ Universal threading processing: ${conversationFetchSuccesses}`);
       console.log(`‚ùå Microsoft conversation API calls: 0 (ELIMINATED)`);
       console.log(`üìä Universal threading success rate: ${universalThreadingSuccessRate}%`);
       console.log(`üöÄ Sync strategy: Phase 3 - Universal RFC2822 threading system (Platform Independent)`);
       console.log(`‚ö° Performance: ~70% faster (eliminated conversation API calls)`);
       console.log(`üéØ Threading: Superior internal notes system active`);
       console.log(`üìé Attachment processing: Smart Reference Architecture with CID normalization`);
       console.log(`üß© Chunk processing: All chunks completed successfully`);
       console.log(`üîÑ Store status: Updated to "Connected"`);
       console.log(`üì° Sidebar refresh: Triggered for real-time UI update`);
       console.log(`‚è±Ô∏è  Total chunk duration: ${debugInfo.totalDuration}ms`);
       console.log('=== END FINAL SYNC STATISTICS ===');
       // ‚ö° FINAL STEP: Update sync_queue status AFTER all comprehensive logging is complete
       // This ensures frontend only updates when everything is truly finished
       console.log('üîÑ [FINAL-UPDATE] Updating sync_queue status to completed - will trigger frontend update...');
       console.log('üîç [BUSINESS-ID-DEBUG] Store business_id for sync_queue update:', store.business_id);
       await supabase.from('sync_queue').update({
         status: 'completed',
         completed_at: new Date().toISOString(),
         emails_processed: allEmails.length,
         business_id: store.business_id,
         metadata: {
           emails_processed: allEmails.length,
           emails_with_attachments: emailsWithAttachments,
           universal_threading_success_rate: universalThreadingSuccessRate + '%',
           phase: 'Phase 3 - Universal RFC2822 Threading',
           performance: '~70% faster than Phase 2',
           chunk_processing: 'all_chunks_completed',
           total_duration_ms: debugInfo.totalDuration
         }
       }).eq('id', parentSyncJobId);
       console.log('‚úÖ [SYNC-QUEUE] Parent sync job marked as completed - frontend will now handle store status update');
       // üè™ ADDITIONAL: Update store table for consistency (backup for real-time subscription)
       console.log('üè™ [STORE-UPDATE] Updating store last_synced for consistency...');
       await supabase.from('stores').update({
         last_synced: new Date().toISOString()
       }).eq('id', storeId);
       console.log('‚úÖ [STORE-UPDATE] Store last_synced updated successfully');
       return new Response(JSON.stringify({
         success: true,
         emailsProcessed: allEmails.length,
         emailsWithAttachments: emailsWithAttachments,
         lastSynced: new Date().toISOString(),
         allChunksCompleted: true,
         isLastChunk: true,
         storeStatusUpdated: true,
         sidebarRefreshTriggered: true,
         debugInfo,
         threadingStats: {
           emailsWithConversationMetadata: conversationFetchAttempts,
           universalThreadingProcessed: conversationFetchSuccesses,
           microsoftApiCalls: 0,
           universalThreadingSuccessRate: universalThreadingSuccessRate + '%',
           phase: 'Phase 3 - Universal RFC2822 Threading',
           performance: '~70% faster than Phase 2',
           features: [
             'Internal Notes',
             'Universal Threading',
             'Platform Independence',
             'RFC2822 Standards',
             'Smart Reference Architecture'
           ]
         },
         attachmentStats: {
           emailsWithAttachments: emailsWithAttachments,
           attachmentProcessingEnabled: true,
           cidExtractionEnabled: true,
           smartReferenceArchitecture: true,
           syntheticAttachmentProcessing: true,
           orphanedCidDetection: true,
           syntheticAttachmentResolution: true,
           multiStrategyResolution: true
         }
       }), {
         status: 200,
         headers: corsHeaders
       });
     } else {
       // üß© CHUNK PROGRESS RESPONSE: For non-final chunks, return minimal progress info
       console.log(`‚úÖ [CHUNK-COMPLETE] Chunk processing completed successfully`);
       console.log(`üìß Emails processed in this chunk: ${allEmails.length}`);
       console.log(`‚è±Ô∏è  Chunk duration: ${debugInfo.totalDuration}ms`);
       console.log(`‚è≥ Waiting for remaining chunks to complete before updating store status...`);
       return new Response(JSON.stringify({
         success: true,
         emailsProcessed: allEmails.length,
         chunkCompleted: true,
         isLastChunk: false,
         allChunksCompleted: false,
         storeStatusUpdated: false,
         awaitingOtherChunks: true,
         debugInfo: {
           ...debugInfo,
           chunkStatus: 'completed',
           finalChunkPending: true
         }
       }), {
         status: 200,
         headers: corsHeaders
       });
     }
   } catch (error) {
     console.error('Error processing email sync:', error);
     return new Response(JSON.stringify({
       error: 'An error occurred while processing the email sync',
       debugInfo
     }), {
       status: 500,
       headers: corsHeaders
     });
   }
 });
 